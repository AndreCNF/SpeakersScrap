{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape main news sources looking for speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from __future__ import unicode_literals, print_function\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important variables\n",
    "config_yaml_dir = \"/Users/AndreCNF/OneDrive/TEDxULisboa/SpeakersScrap/configs/andreferreira_yaml_0.1.yaml\"\n",
    "current_date = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yaml configuration file, with the requested news sources\n",
    "with open(config_yaml_dir, 'r') as stream:\n",
    "    try:\n",
    "        config_yaml = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['newspapers'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each news company\n",
    "for company, value in tqdm.tqdm(config_yaml['news_sources'].items()):\n",
    "    print(\"Building site for \", company)\n",
    "    \n",
    "    # Counting the number of articles read from a news source company\n",
    "    count = 1\n",
    "    \n",
    "    paper = newspaper.build(value['link'], memoize_articles=False)\n",
    "    newsPaper = {\n",
    "        \"link\": value['link'],\n",
    "        \"articles\": []\n",
    "    }\n",
    "    \n",
    "    # Counting the number of articles without a readable publish date\n",
    "    noneTypeCount = 0\n",
    "    \n",
    "    for content in paper.articles:\n",
    "        if count > LIMIT:\n",
    "            break\n",
    "        try:\n",
    "            content.download()\n",
    "            content.parse()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"continuing...\")\n",
    "            continue\n",
    "            \n",
    "        # Again, for consistency, if there is no found publish date the article will be skipped.\n",
    "        # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n",
    "        if content.publish_date is None:\n",
    "            print(count, \" Article has date of type None...\")\n",
    "            noneTypeCount = noneTypeCount + 1\n",
    "            if noneTypeCount > 10:\n",
    "                print(\"Too many noneType dates, aborting...\")\n",
    "                noneTypeCount = 0\n",
    "                break\n",
    "            count = count + 1\n",
    "            continue\n",
    "            \n",
    "        # Ignore news articles older than a day ago\n",
    "        elif content.publish_date < current_date - timedelta(days=1):\n",
    "            break\n",
    "            \n",
    "        article = {}\n",
    "        article['title'] = content.title\n",
    "        article['text'] = content.text\n",
    "        article['link'] = content.url\n",
    "        article['published'] = content.publish_date.isoformat()\n",
    "        content.nlp()\n",
    "        article['keywords'] = content.keywords\n",
    "        article['summary'] = content.summary\n",
    "        newsPaper['articles'].append(article)\n",
    "        print(count, \"articles downloaded from\", company, \"using newspaper, url:\", content.url)\n",
    "        count = count + 1\n",
    "        noneTypeCount = 0\n",
    "\n",
    "        data['newspapers'][company] = newsPaper\n",
    "        \n",
    "try:\n",
    "    with open('results/scraped_articles.yaml', 'w') as outfile:\n",
    "        yaml.dump(data, outfile, allow_unicode=True)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to Google Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

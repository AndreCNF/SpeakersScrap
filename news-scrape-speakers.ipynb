{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape main news sources looking for speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from __future__ import unicode_literals, print_function\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import newspaper\n",
    "import yaml\n",
    "import tqdm\n",
    "import pendulum\n",
    "from langdetect import detect\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important variables\n",
    "config_yaml_dir = \"configs/andreferreira_yaml_0.1.yaml\"\n",
    "current_date = pendulum.now('Europe/Lisbon')\n",
    "model_dir = 'NLP/models/speakers_model_2018_09_23_02_50_37'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use creds to create a client to interact with the Google Drive API\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('client_secret.json', scope)\n",
    "client = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom NLP model trained to find articles about good speakers\n",
    "nlp = spacy.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp_en = spacy.load('en')\n",
    "\n",
    "# Load Portuguese tokenizer, tagger, parser, NER and word vectors\n",
    "nlp_pt = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yaml configuration file, with the requested news sources\n",
    "with open(config_yaml_dir, 'r') as stream:\n",
    "    try:\n",
    "        config_yaml = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the limit for number of articles to download, per news source\n",
    "LIMIT = 20\n",
    "\n",
    "data = {}\n",
    "data['newspapers'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each news company\n",
    "for company, value in tqdm.tqdm(config_yaml['news_sources'].items()):\n",
    "    print(\"Building site for \", company)\n",
    "    \n",
    "    # Counting the number of articles read from a news source company\n",
    "    count = 1\n",
    "    \n",
    "    paper = newspaper.build(value['link'], memoize_articles=False)\n",
    "    newsPaper = {\n",
    "        \"link\": value['link'],\n",
    "        \"articles\": []\n",
    "    }\n",
    "    \n",
    "    # Counting the number of articles without a readable publish date\n",
    "    noneTypeCount = 0\n",
    "    \n",
    "    for content in paper.articles:\n",
    "        if count > LIMIT:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            content.download()\n",
    "            content.parse()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"continuing...\")\n",
    "            count = count + 1\n",
    "            continue\n",
    "            \n",
    "        # Ignore short texts\n",
    "        if len(content.text) < 280:\n",
    "            print(\"Skipping text of length \" + str(len(content.text)))\n",
    "            count = count + 1\n",
    "            continue\n",
    "            \n",
    "        lang = detect(content.text)\n",
    "        \n",
    "        # Ignore texts written in a language that's not portuguese\n",
    "        if lang != 'pt':\n",
    "            print(\"Ignoring text that is written in \" + lang + \" language.\")\n",
    "            count = count + 1\n",
    "            continue\n",
    "            \n",
    "        # Use the correct language model to find mentions of people\n",
    "        if lang == 'pt':\n",
    "            nlp_lang = nlp_pt\n",
    "        elif lang == 'en':\n",
    "            nlp_lang = nlp_en\n",
    "            \n",
    "        people_list = []\n",
    "            \n",
    "        # Get the list of people mentioned in the text\n",
    "        for entity in nlp_lang(content.text).ents:\n",
    "            if 'PER' in entity.label_:\n",
    "                people_list.append(entity.text)\n",
    "                \n",
    "        if len(people_list) == 0:\n",
    "            print(\"Ignoring text as no mention to people was found.\")\n",
    "            count = count + 1\n",
    "            continue\n",
    "            \n",
    "        # Again, for consistency, if there is no found publish date the article will be skipped.\n",
    "        # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n",
    "        if content.publish_date is None:\n",
    "            print(count, \" Article has date of type None...\")\n",
    "            noneTypeCount = noneTypeCount + 1\n",
    "            if noneTypeCount > 10:\n",
    "                print(\"Too many noneType dates, aborting...\")\n",
    "                noneTypeCount = 0\n",
    "                break\n",
    "            count = count + 1\n",
    "            continue\n",
    "            \n",
    "        # Get yesterday's date, at the same time (hours, minutes, seconds) as now\n",
    "        yesterday = current_date - timedelta(days=1)\n",
    "            \n",
    "        # If the current article doesn't have a timezone specified, ignore our timezone info to avoid problems\n",
    "        if content.publish_date.tzinfo == None:\n",
    "            yesterday = yesterday.replace(tzinfo=None)\n",
    "            \n",
    "        # Ignore news articles older than a day ago\n",
    "        elif content.publish_date < yesterday:\n",
    "            print(\"Skipping article from \" + str(content.publish_date))\n",
    "            count = count + 1\n",
    "            continue\n",
    "            \n",
    "        # Score given by the NLP model, indicating the probability that it thinks that the\n",
    "        # article mentions a good speaker.\n",
    "        nlp_score = nlp(content.text).cats['POSITIVE']\n",
    "            \n",
    "        # Ignore news articles with a bad NLP score\n",
    "        if nlp_score < 0.8:\n",
    "            print(\"Ignoring article with an NLP score of \" + str(nlp_score))\n",
    "            count = count + 1\n",
    "            continue\n",
    "            \n",
    "        article = {}\n",
    "        article['title'] = content.title\n",
    "        article['text'] = content.text\n",
    "        article['link'] = content.url\n",
    "        article['published'] = content.publish_date.isoformat()\n",
    "        content.nlp()\n",
    "        article['keywords'] = content.keywords\n",
    "        article['summary'] = content.summary\n",
    "        \n",
    "        # Add the names of people mentioned in the text\n",
    "        article['people'] = people_list\n",
    "        \n",
    "        # Score given by the NLP model, indicating the probability that it thinks that the\n",
    "        # article mentions a good speaker.\n",
    "        article['nlp_score'] = nlp_score\n",
    "        \n",
    "        # Add article data to the news source's list\n",
    "        newsPaper['articles'].append(article)\n",
    "        print(count, \"articles downloaded from\", company, \"using newspaper, previous article's date: \" + \n",
    "              content.publish_date.isoformat() + \", url:\", content.url)\n",
    "        count = count + 1\n",
    "        noneTypeCount = 0\n",
    "\n",
    "        # Add the current news source's articles data to the whole news list\n",
    "        data['newspapers'][company] = newsPaper\n",
    "        \n",
    "try:\n",
    "    articles_dir = 'results/scraped_articles_' + str(current_date).replace(' ', '_').replace('-', '_') + '.yaml'\n",
    "    \n",
    "    with open(articles_dir, 'w') as outfile:\n",
    "        yaml.dump(data, outfile, allow_unicode=True)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to Google Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file with the spreadsheets\n",
    "file = client.open(\"Research Speakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a specific spreadsheet from the file\n",
    "bot_sheet = file.worksheet(\"bot_oradores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of filled rows (ignoring the first row as it's an header)\n",
    "n_rows = len(bot_sheet.col_values(1)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yaml from previous scraping\n",
    "with open('results/scraped_articles_2018_09_24T00:54:42.275428+01:00.yaml', 'r') as stream:\n",
    "    try:\n",
    "        results_yaml = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 2 instead of just 1 to the length of rows, as the header had been ignored\n",
    "# when calculating the number of rows\n",
    "row = n_rows + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for newspaper, articles in tqdm.tqdm(results_yaml['newspapers'].items()):\n",
    "    print('Going through the articles scraped from ' + newspaper)\n",
    "    \n",
    "    for vals in articles['articles']:\n",
    "        if vals['nlp_score'] > 0.8:\n",
    "            # Insert the names\n",
    "            bot_sheet.update_cell(row, 1, str(vals['people']))\n",
    "            \n",
    "            # Wait 1s to avoid Google Sheets API restrictions\n",
    "            sleep(1)\n",
    "            \n",
    "            bot_sheet.update_cell(row, 2, vals['nlp_score'])\n",
    "            \n",
    "            # Wait 1s to avoid Google Sheets API restrictions\n",
    "            sleep(1)\n",
    "            \n",
    "            bot_sheet.update_cell(row, 3, vals['title'])\n",
    "            \n",
    "            # Wait 1s to avoid Google Sheets API restrictions\n",
    "            sleep(1)\n",
    "            \n",
    "            bot_sheet.update_cell(row, 4, vals['summary'])\n",
    "            \n",
    "            # Wait 1s to avoid Google Sheets API restrictions\n",
    "            sleep(1)\n",
    "            \n",
    "            bot_sheet.update_cell(row, 5, newspaper)\n",
    "            \n",
    "            # Wait 1s to avoid Google Sheets API restrictions\n",
    "            sleep(1)\n",
    "            \n",
    "            bot_sheet.update_cell(row, 6, vals['published'])\n",
    "            \n",
    "            # Wait 1s to avoid Google Sheets API restrictions\n",
    "            sleep(1)\n",
    "            \n",
    "            bot_sheet.update_cell(row, 7, vals['link'])\n",
    "            \n",
    "            # Move on to the next row of the spreadsheet\n",
    "            row = row + 1\n",
    "            \n",
    "            # Wait 1s to avoid Google Sheets API restrictions\n",
    "            sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
